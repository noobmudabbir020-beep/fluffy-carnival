import os
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq  # ğŸ‘ˆ using Groq instead of OpenAI

# ----------------------------
# CONFIG
# ----------------------------

os.environ["GROQ_API_KEY"] = "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"
os.environ["HF_TOKEN"] = "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"

# Embedding model
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
# LLM model
LLM_MODEL = "llama-3.3-70b-versatile"
# Knowledge file
FILE_PATH = "knowledge.txt"

# ----------------------------
# LOAD EMBEDDINGS
# ----------------------------
@st.cache_resource
def load_vectordb():
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)

    if not os.path.exists(FILE_PATH):
        raise FileNotFoundError("ğŸ“‚ knowledge.txt not found. Please create it!")

    with open(FILE_PATH, "r", encoding="utf-8") as f:
        content = f.read().strip()

    if not content:
        raise ValueError("ğŸ“‚ knowledge.txt is empty. Add some text first âœï¸")

    # ğŸ‘‡ split into small chunks for embeddings
    texts = content.split(". ")
    vectordb = Chroma.from_texts(texts, embeddings)
    return vectordb


vectordb = load_vectordb()

# ----------------------------
# SET UP RETRIEVER + LLM
# ----------------------------
retriever = vectordb.as_retriever(search_kwargs={"k": 3})
llm = ChatGroq(model=LLM_MODEL, temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# ----------------------------
# STREAMLIT UI
# ----------------------------
st.title("ğŸ’¡ Local Knowledge Q&A Bot ğŸ¤–")

st.write("Ask me questions based on your `knowledge.txt` file ğŸ“„")

user_q = st.text_input("â“ Enter your question:")

if user_q:
    with st.spinner("ğŸ¤” Thinking..."):
        response = qa_chain.invoke(user_q)

    st.subheader("âœ… Answer:")
    st.write(response["result"])

    st.subheader("ğŸ“š Sources:")
    for doc in response["source_documents"]:
        st.write(f"- {doc.page_content}")